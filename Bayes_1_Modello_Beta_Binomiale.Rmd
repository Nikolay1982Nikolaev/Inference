---
title: "Bayes_1_Modello_Beta_Binomiale"
author: "NikolayNikolaev"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Nell'inferenza Bayesiana si formula un'ipotesi detta a priori circa la distribuzione
dei parametri del modello.

L'inferenza e' basata sulla distribuzione a
posteriori che e' la distribuzione che coniuga l'informazione a priori con quella
fornita al parametro dal supporto dei dati osservati (verosimiglianza).
- $p(\theta)$ - a priori per parametro $\theta$
- $L(\theta) = \prod_{i=1}^n p(y_i |\theta)$ - verosimiglianza

$$ p(\theta|y) = \frac{\prod_{i=1}^n p(y_i|\theta).p(\theta)}{\int_{\Theta} \prod_{i=1}^n  p(y_i|\theta).p(\theta) dx}$$
- $\int_{\Theta} \prod_{i=1}^n  p(y_i|\theta).p(\theta) dx$ - distribuzione marginale

Distribuzione a posteriori = (Verosimiglianza x distribuzione a priori) / distribuzione marginale

L'approssimazione: dist.Posteriori = (proporzione)= Verosimiglianzi x distribuzione a priori

La probabilita iniziale (a priori) e' la funzione di densita/probabilita del vettore
dei parametri che dipende (e' condizionata) dall'informazione iniziale (nel
seguito quest'informazione iniziale si denota come H) e la prior come $p(\theta|H)$


##  MODELLI CONIUGATI
- ovvero quelli in cui la distribuzione a priori e quella a posteriori appartengono alla
stessa famiglia di densita o probabilita.









# Modello Beta-Binomiale:
Si consideri la proporzione di un certo evento nella popolazione in base a n
osservazioni campionarie indipendenti. Il numero di successi osservati sono
assunti come determinazioni da una variabile casuale Binomiale. Nell'ambito
dell'inferenza Bayesiana la probabilta di successo $\theta$ e' caratterizzata da una distribuzione a priori che deve essere definita in [1,0]

Una scelta ragionevole per la prior e' distribuzione $Beta(\alpha, \beta)$

data che i parametri di interesse e' $\theta$ i parameti della distribuzione a priori sono definiti IPERparametri:
$$p(\theta) = \frac{1}{Beta(\alpha, \beta).\theta^{\alpha-1} . (1-\theta)^{\beta-1}}$$
- $0=< \theta =< 1$

Il modello di riferimento per i conteggi e' defnito dalla distribuzione Binomiale
che generalizza la distribuzione di Bernoulli dove ogni singola prova
ha probabilita costante di successo $\theta$. Sotto l'ipotesi di variabili casuali siano
indipendenti e identicamente distribuite il modello statistico diventa il
seguente
$$(X_1...X_n|\theta) = Bernoulli(\theta)$$

$$P(X=x|\Theta=\theta) = \theta^x(1-\theta)^{1-x}$$

- x = 0,1

Se si dispone di n realizzazioni indipendenti da un campione casuale $x =
(x_1; x_2; ... ; x_n)$ la funzione di verosimiglianza
$$ l(\theta, x) = \prod_{i=1}^n [\theta^x(1-\theta)^{1-x_i}] = \theta^k (1-\theta)^{n-k}$$

- $k =\sum_{i=1}^n x_i$ - numero di sucessi

La distribuzione Beta e la distribuzione Binomiale sono legate, infatti si puo
pensare al parametro $\alpha$ della distribuzione Beta come il numero di sucessi della distribuzione Binomiale a cui si aggiunge uno $k+1$ e al parametro $\beta$ della distribuzione. Betaq come il numero di insucessi cui si aggiunge uno $n-k+_1$ overo $Beta(k+1, n-k+1)$

La distribuzione Beta e' distribuzuione a priori coniugata alla distribuzione
Binomiale perche la prior e la posterior hanno stessa forma distributiva.
Infatti applicando la regola di Bayes anche la distribuzione a posteriori e' una
Beta. Essendo


$$p(\theta) prporzionale -p(\theta).l(\theta, x)$$


$$p(\theta|x) -proporzionale-- \theta^{k+\alpha -1}.(1-\theta)^{n-k+\beta-1}$$

La stima Bayesiana della probabilita si eettua considerando il valore atteso
della distribuzione a posteriori

$$\frac{k+\alpha}{n+\alpha+\beta} = \frac{k}{n}(\frac{n}{n+\alpha+\beta}) + \frac{\alpha}{\alpha + \beta}.(\frac{\alpha+\beta}{n+\alpha+\beta})$$

ovvero si tratta di una media pesata della proporzione campionaria dei successi
e della media della distribuzione a priori.

- altrimenti come stima puntuale si puo usare la moda della distribuzione a
posteriori che e' data da
$$\frac{k+\alpha-1}{n+\alpha+\beta-2}$$

Si dimostra che la costante di normalizzazione e' data da

$$Beta(\alpha, \beta)^{-1} * Beta(\alpha+k, \beta+n - k)^{-1}$$

La distribuzione a posteriori e' proporzionale al prodotto della verosimiglianza e della prior pertanto


$$p(\theta|x) = proporzionale == \theta^x(1-\theta)^{n-x} *   \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{k+\alpha-1}(1-\theta)^{n-k+\beta-1} $$
Il modello coniugato e' defnito Beta-Binomiale.


Si dimostra che la media a posteriori e una media pesata tra la media della
distribuzione a priori e la proporzione campionaria $\hat{\theta_{ML}} = \frac{k}{n}$ (stima di
massima verosimiglianza) ed il peso dipende dalla numerosita campionaria.


La versomiglianza ha piu peso rispetto alla distribuzione a priori quando il
campione e' particolarmente ampio






In un campione di 20 (n) pazienti, 13 (k) pazienti sono ipertesi. SI ipotizza un modello Beta 
Binomiale per stimare la proporzione di pazienti nella popolazione di riferimento.

Si disegna per stimare a posteriori in base a diverse formulazioni della distribuzione a priori.

Si considerano i seguenti tre scenari
1. distribuzione a priori uniforme $\alpha=1, \beta=1$
2. distribuzione a priori con $\alpha, \beta <1$
3. distribuzione a priori con $\alpha, \beta > 1$

## Scenario 1:

Sappendo che la distribuzione a priori per il parametro $\theta$ che rappresenta la probabilita di successi (ipertesi) sia una $Beta(\alpha, \beta)$ tale che $\alpha=\beta = 1$.
  
La verosimiglianza e' una Binomiale in cui kernel e' proporzionale alla Beta con parametri $\binom{\alpha = k+1}{\beta=n-k+1}$ (dispensa di teoria)

La distribuzione a posteriori e' ancora una Beta con parametri $(\alpha + k)$ and $(\beta +n -k)$

Nel seguente rafico si visualizzano le tre distribuzioni:
---
```{r}
alpha <- 1
beta  <- 1
pval  <-seq(0,1,by=0.01)

## distribuzione a priori
plot(pval, dbeta(pval,alpha,beta),
type = "l", col = "blue", ylim = c(0,6),
ylab = "Densità", xlab = "theta",
main = "Scenario 1")

## verosimiglianza
n <- 20
k <- 13
lines(pval, dbeta(pval, k+1, n-k+1),
      lwd = 2,
col = "darkblue")
## distribuzione a posteriori
lines(pval, dbeta(pval, k+alpha, n-k+beta),
      lwd = 1,
      col = "red")
legend("topleft",
       c("a Priori","Verosimiglianza","a Posteriori"),
       lty=c(1,1,1),
       lwd=c(1,2,1),
       col=c( "blue","darkblue","red" ), cex=0.6)
```


La numerosità campionaria è esigua. Si evince che nel caso in cui la prior non è informativa
(scenario 1) la distribuzione a posteriori coincide con la verosimiglianza.

Il valore atteso della distribuzione a posteriori costistituisce la stima puntuale Bayesiana per il 
parametro.
```{r}
(alpha + k)/(n+alpha + beta)
```
la proporzione di ipertesi nella popolazione è 0.64.

La moda della distribuzione a priori è un’altra stima puntuale per il parametro e coincide
con la stima di massima verosimiglianza
```{r}
(alpha + k -1)/(n + alpha + beta -2)
```

## Scenario 2:
Scegliendo come parametri della distribuzione a priori quelli proposti da Jeffrey ovvero $\alpha = 0.5$
and $\beta=0.5$ la prior non e' molto informativa.

```{r}
alpha <- 0.5
beta<- 0.5
# distribuzione a priori
pval<-seq(0,1,by=0.01)
plot(pval, dbeta(pval,alpha,beta),
type = "l", col = "blue", ylim = c(0,6),
ylab = "Densità", xlab = "theta", main = "Scenario 2")
# verosimiglianza
lines(pval, dbeta(pval, k+1, n-k+1),
lwd = 2,
col = "darkblue")
# distribuzione a posteriori
lines(pval, dbeta(pval, k+alpha, n-k+beta),
lwd = 1,
col = "red")
legend("topleft",
c("a Priori","Verosimiglianza","a Posteriori"),
lty=c(1,1,1),
lwd=c(1,2,1),
col=c( "blue","darkblue","red" ), cex=0.6)

```
In questo scenario in cui la prior è un po’ più informativa si nota la diversa forma della 
distribuzione a priori e la traslazione della a posteriori verso destra rispetto alla verosimiglianza.

Il valore atteso adesso è
```{r}
(alpha + k)/(n+alpha + beta)
```

La moda è leggermente superiore a quella osservata per lo scenario 1

```{r}
(alpha + k -1)/(n + alpha + beta -2)
```

## Scenario 3:
Nel caso in cui si suppone una distribuzione a priori *molto informativa* ad esempio con $\alpha=3.26$
and $\beta = 7.19$ si ha:

```{r}
alpha <- 3.26
beta<- 7.19
# distribuzione a priori
pval<-seq(0,1,by=0.01)
plot(pval, dbeta(pval,alpha,beta),
type = "l", col = "blue", ylim = c(0,6),
ylab = "Densità", xlab = "theta", main = "Scenario 3")
# verosimiglianza
lines(pval, dbeta(pval, k+1, n-k+1),
lwd = 2,
col = "darkblue")
# distribuzione a posteriori
lines(pval, dbeta(pval, k+alpha, n-k+beta),
lwd = 1,
col = "red")
legend("topleft",
c("a Priori","Verosimiglianza","a Posteriori"),
lty=c(1,1,1),
lwd=c(1,2,1),
col=c( "blue","darkblue","red" ), cex=0.6)

```
In questo caso la distribuzione a priori è molto infomativa e pertanto “trascina” verso la sua
moda la distribuzione a posteriori che è sempre spostata verso la verosimiglianza ma meno
vicina a quest’ultima rispetto ai due casi precenti.
La stima puntuale della probabilità di successo in base al valore atteso della distribuzione a
posteriori è
```{r}
(alpha + k)/(n+alpha + beta)

```

Anche la stima in base alla moda è inferiore rispetto a quelle dello scenario 1 e 2

```{r}
(alpha + k -1)/(n + alpha + beta -2)
```

Nello scenario considerato la distribuzione a priori è piuttosto informativa ed essendo la
numerosità campionaria non troppo elevata la distribuzione a posteriori risulta influenzata
maggiormente dalla distribuzione a priori, il peso della funzione di verosimiglianza è minore.

## Example: Bayes Billiard Balls:
